{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Multimodal Housing Price Prediction\n",
        "Problem Statement & Objective: Predict real estate prices by fusing two different data modalities: structured tabular data and unstructured image data.\n",
        "\n",
        "Dataset Loading & Preprocessing: Used the Houses Dataset. Normalized tabular features with MinMaxScaler and resized/normalized house images to (64, 64) pixels.\n",
        "\n",
        "Model Development & Training: Developed a dual-branch neural network: a CNN for image feature extraction and an MLP for tabular data. Fused the branches using the Keras concatenate layer.\n",
        "\n",
        "Evaluation Metrics: Measured performance using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) in actual dollar amounts.\n",
        "\n",
        "Visualizations: Model architecture plot and Loss curves showing convergence over 50 epochs.\n",
        "\n",
        "Final Summary / Insights: Multimodal models provide a richer understanding of data; visual features (house appearance) significantly complement numerical data (square footage) for price estimation."
      ],
      "metadata": {
        "id": "Tfi2hhvlxOwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26b6YUcDRSgm",
        "outputId": "d4b73575-cfc5-403c-9f47-24a9d26b2d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow opencv-python pandas scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Acquisition and Preprocessing"
      ],
      "metadata": {
        "id": "VnA6Yif0RU24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 1. Clone the dataset\n",
        "if not os.path.exists(\"Houses-dataset\"):\n",
        "    !git clone https://github.com/emanhamed/Houses-dataset\n",
        "\n",
        "# 2. Load Tabular Data\n",
        "cols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "df = pd.read_csv(\"Houses-dataset/Houses Dataset/HousesInfo.txt\", sep=\" \", header=None, names=cols)\n",
        "\n",
        "# 3. Load Images (Using the 'Frontal' image for each house)\n",
        "images = []\n",
        "for i in df.index.values:\n",
        "    # Path to the frontal image\n",
        "    basePath = os.path.sep.join([\"Houses-dataset/Houses Dataset\", \"{}_frontal.jpg\".format(i + 1)])\n",
        "    image = cv2.imread(basePath)\n",
        "    image = cv2.resize(image, (64, 64)) # Resize for memory efficiency\n",
        "    images.append(image)\n",
        "\n",
        "images = np.array(images) / 255.0 # Normalize pixel values\n",
        "\n",
        "# 4. Preprocess Tabular Data\n",
        "# Scaling the target (price) and features\n",
        "scaler = MinMaxScaler()\n",
        "X_tab = scaler.fit_transform(df.drop(\"price\", axis=1))\n",
        "y = df[\"price\"].values / df[\"price\"].max() # Normalize price for training stability\n",
        "\n",
        "# 5. Split into Train/Test\n",
        "split = train_test_split(X_tab, images, y, test_size=0.2, random_state=42)\n",
        "(trainTab, testTab, trainImg, testImg, trainY, testY) = split"
      ],
      "metadata": {
        "id": "W6j0nUU0RXiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Multimodal Architecture"
      ],
      "metadata": {
        "id": "chDKFPw9Radw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, concatenate\n",
        "\n",
        "# BRANCH 1: CNN (Image processing)\n",
        "img_input = Input(shape=(64, 64, 3))\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding=\"same\")(img_input)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding=\"same\")(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(16, activation=\"relu\")(x)\n",
        "cnn_branch = Model(inputs=img_input, outputs=x)\n",
        "\n",
        "# BRANCH 2: MLP (Tabular processing)\n",
        "tab_input = Input(shape=(4,))\n",
        "y_dense = Dense(16, activation=\"relu\")(tab_input)\n",
        "y_dense = Dense(8, activation=\"relu\")(y_dense)\n",
        "mlp_branch = Model(inputs=tab_input, outputs=y_dense)\n",
        "\n",
        "# FUSION: Combine both branches\n",
        "combined = concatenate([cnn_branch.output, mlp_branch.output])\n",
        "\n",
        "# Final Regression Layers\n",
        "z = Dense(4, activation=\"relu\")(combined)\n",
        "z = Dense(1, activation=\"linear\")(z)\n",
        "\n",
        "# Final Multimodal Model\n",
        "model = Model(inputs=[cnn_branch.input, mlp_branch.input], outputs=z)\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])"
      ],
      "metadata": {
        "id": "24vIpM8LReTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model"
      ],
      "metadata": {
        "id": "uaAb6OSJRe76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[INFO] Training multimodal model...\")\n",
        "history = model.fit(\n",
        "    x=[trainImg, trainTab], y=trainY,\n",
        "    validation_data=([testImg, testTab], testY),\n",
        "    epochs=50, batch_size=8\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxy_ahNjRi1x",
        "outputId": "cd33c7c4-4c51-4563-90c7-44f03ceaeb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training multimodal model...\n",
            "Epoch 1/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0119 - mae: 0.0823 - val_loss: 0.0043 - val_mae: 0.0510\n",
            "Epoch 2/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 52ms/step - loss: 0.0094 - mae: 0.0590 - val_loss: 0.0040 - val_mae: 0.0475\n",
            "Epoch 3/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0102 - mae: 0.0601 - val_loss: 0.0039 - val_mae: 0.0459\n",
            "Epoch 4/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.0074 - mae: 0.0548 - val_loss: 0.0038 - val_mae: 0.0482\n",
            "Epoch 5/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 0.0116 - mae: 0.0592 - val_loss: 0.0036 - val_mae: 0.0444\n",
            "Epoch 6/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.0073 - mae: 0.0536 - val_loss: 0.0036 - val_mae: 0.0472\n",
            "Epoch 7/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 0.0056 - mae: 0.0516 - val_loss: 0.0031 - val_mae: 0.0420\n",
            "Epoch 8/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.0063 - mae: 0.0488 - val_loss: 0.0034 - val_mae: 0.0482\n",
            "Epoch 9/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.0067 - mae: 0.0505 - val_loss: 0.0044 - val_mae: 0.0573\n",
            "Epoch 10/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0041 - mae: 0.0440 - val_loss: 0.0034 - val_mae: 0.0475\n",
            "Epoch 11/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0062 - mae: 0.0491 - val_loss: 0.0030 - val_mae: 0.0430\n",
            "Epoch 12/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0043 - mae: 0.0416 - val_loss: 0.0034 - val_mae: 0.0473\n",
            "Epoch 13/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step - loss: 0.0027 - mae: 0.0348 - val_loss: 0.0057 - val_mae: 0.0623\n",
            "Epoch 14/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.0024 - mae: 0.0361 - val_loss: 0.0030 - val_mae: 0.0406\n",
            "Epoch 15/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.0025 - mae: 0.0363 - val_loss: 0.0035 - val_mae: 0.0443\n",
            "Epoch 16/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - loss: 0.0026 - mae: 0.0326 - val_loss: 0.0039 - val_mae: 0.0473\n",
            "Epoch 17/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0025 - mae: 0.0330 - val_loss: 0.0048 - val_mae: 0.0529\n",
            "Epoch 18/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0024 - mae: 0.0321 - val_loss: 0.0046 - val_mae: 0.0497\n",
            "Epoch 19/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 0.0016 - mae: 0.0275 - val_loss: 0.0043 - val_mae: 0.0483\n",
            "Epoch 20/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.0017 - mae: 0.0262 - val_loss: 0.0034 - val_mae: 0.0411\n",
            "Epoch 21/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.0010 - mae: 0.0241 - val_loss: 0.0080 - val_mae: 0.0670\n",
            "Epoch 22/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.0012 - mae: 0.0263 - val_loss: 0.0043 - val_mae: 0.0459\n",
            "Epoch 23/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 9.0795e-04 - mae: 0.0214 - val_loss: 0.0046 - val_mae: 0.0472\n",
            "Epoch 24/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 5.4728e-04 - mae: 0.0170 - val_loss: 0.0041 - val_mae: 0.0440\n",
            "Epoch 25/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 7.0129e-04 - mae: 0.0199 - val_loss: 0.0060 - val_mae: 0.0564\n",
            "Epoch 26/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 7.4794e-04 - mae: 0.0195 - val_loss: 0.0043 - val_mae: 0.0457\n",
            "Epoch 27/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 4.9409e-04 - mae: 0.0162 - val_loss: 0.0043 - val_mae: 0.0460\n",
            "Epoch 28/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 4.1069e-04 - mae: 0.0152 - val_loss: 0.0044 - val_mae: 0.0460\n",
            "Epoch 29/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 5.2395e-04 - mae: 0.0156 - val_loss: 0.0042 - val_mae: 0.0452\n",
            "Epoch 30/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 4.0659e-04 - mae: 0.0145 - val_loss: 0.0045 - val_mae: 0.0465\n",
            "Epoch 31/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 4.0724e-04 - mae: 0.0137 - val_loss: 0.0043 - val_mae: 0.0454\n",
            "Epoch 32/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 4.2492e-04 - mae: 0.0135 - val_loss: 0.0048 - val_mae: 0.0482\n",
            "Epoch 33/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 3.7201e-04 - mae: 0.0121 - val_loss: 0.0050 - val_mae: 0.0507\n",
            "Epoch 34/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 3.7357e-04 - mae: 0.0134 - val_loss: 0.0046 - val_mae: 0.0467\n",
            "Epoch 35/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 3.2378e-04 - mae: 0.0124 - val_loss: 0.0047 - val_mae: 0.0477\n",
            "Epoch 36/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 2.7477e-04 - mae: 0.0113 - val_loss: 0.0047 - val_mae: 0.0479\n",
            "Epoch 37/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 3.1673e-04 - mae: 0.0113 - val_loss: 0.0046 - val_mae: 0.0472\n",
            "Epoch 38/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 4.3313e-04 - mae: 0.0123 - val_loss: 0.0044 - val_mae: 0.0456\n",
            "Epoch 39/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 3.5813e-04 - mae: 0.0130 - val_loss: 0.0046 - val_mae: 0.0476\n",
            "Epoch 40/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 3.2069e-04 - mae: 0.0117 - val_loss: 0.0044 - val_mae: 0.0461\n",
            "Epoch 41/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 3.5630e-04 - mae: 0.0138 - val_loss: 0.0048 - val_mae: 0.0489\n",
            "Epoch 42/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 3.3549e-04 - mae: 0.0133 - val_loss: 0.0047 - val_mae: 0.0484\n",
            "Epoch 43/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - loss: 3.3711e-04 - mae: 0.0134 - val_loss: 0.0043 - val_mae: 0.0458\n",
            "Epoch 44/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 4.0168e-04 - mae: 0.0143 - val_loss: 0.0048 - val_mae: 0.0481\n",
            "Epoch 45/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 51ms/step - loss: 5.1020e-04 - mae: 0.0170 - val_loss: 0.0060 - val_mae: 0.0547\n",
            "Epoch 46/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 5.4782e-04 - mae: 0.0174 - val_loss: 0.0052 - val_mae: 0.0530\n",
            "Epoch 47/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 5.0227e-04 - mae: 0.0169 - val_loss: 0.0042 - val_mae: 0.0447\n",
            "Epoch 48/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 5.4696e-04 - mae: 0.0160 - val_loss: 0.0045 - val_mae: 0.0464\n",
            "Epoch 49/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 3.5548e-04 - mae: 0.0139 - val_loss: 0.0037 - val_mae: 0.0417\n",
            "Epoch 50/50\n",
            "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 3.8640e-04 - mae: 0.0143 - val_loss: 0.0046 - val_mae: 0.0463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Performance (MAE and RMSE)"
      ],
      "metadata": {
        "id": "7Bh2fC6fRjZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# 1. Make predictions\n",
        "preds = model.predict([testImg, testTab])\n",
        "\n",
        "# 2. Rescale predictions and actual values back to original prices\n",
        "maxPrice = df[\"price\"].max()\n",
        "final_preds = preds.flatten() * maxPrice\n",
        "final_actual = testY * maxPrice\n",
        "\n",
        "# 3. Calculate Metrics\n",
        "mae = mean_absolute_error(final_actual, final_preds)\n",
        "rmse = np.sqrt(mean_squared_error(final_actual, final_preds))\n",
        "\n",
        "print(f\"\\n--- Model Evaluation ---\")\n",
        "print(f\"Mean Absolute Error (MAE): ${mae:,.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): ${rmse:,.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdWdt6dlRmMj",
        "outputId": "2c6bc32b-404f-4d3e-ba31-d85a23cd6019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\n",
            "--- Model Evaluation ---\n",
            "Mean Absolute Error (MAE): $271,292.83\n",
            "Root Mean Squared Error (RMSE): $395,150.87\n"
          ]
        }
      ]
    }
  ]
}